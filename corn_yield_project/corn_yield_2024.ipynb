{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script aims at **forecasting the 2024 U.S. national corn yield (bu/acre)** using historical USDA yield data and provided weather data.\n",
    "\n",
    "The corn data are obtained with API and downloaded automatically.\n",
    "\n",
    "Only the category \"ALL PRODUCTION PRACTICES\" (see USDA agric. database website) has been considered (thus there is no difference between irrigated and non-irrigated fields). --> The model could be relaxed to actually discriminate between the two cases, as we expect different weather features to be important.\n",
    "\n",
    "Only the counties for which the weather was available have been considered in the model. As a next step for including the other counties there is a first easy move:\n",
    "We can look for the trend of each of these counties (or aggregated counties under the name of \"OTHER (AGGREGATE)\" under the USDA data) and then we can model the remaining fluctuations as a random walk.\n",
    "We can then do some quick Monte Carlo modeling for including these counties in the forecast.\n",
    "However, I expect the weight of these datasets to be small as otherwise they would not be aggregated. Furthermore, if counties are far away (especially in big states) a real modeling would be complicated as each underlying random process could mix and be undetectable.\n",
    "\n",
    "Before developing any model I find very constructive and insightful to look at the data and perform some correlations to have a first hint. I started comparing the corn yield among different states.\n",
    "\n",
    "The first interesting result is that the standard deviation can be extremely different among the different states. An example is **Ohio** and **South Dakota**. Thus it appeared clear that probably each state must be modeled independently to reduce the variance of the prediction. Thus I have done some weather plots for Ohio and South Dakota to see if already graphically it was easy to detect the differences. [*some weather considerations..*]\n",
    "\n",
    "After this first investigation, I focused on **feature engineering** and **corn data transformation**.\n",
    "\n",
    "In particular from the weather dataset I extracted some values that could be relevant for the corn yield modeling. The list of these features is in the first cell of the notebook. Some of the features are the **maximum continus days without rains**, the **max temperature in the month**, **very cold day** (T<-5 degree>) and **very hot days** (T>35 degrees) and I selected them due to the previous observations and differences between Ohio and South Dakota.\n",
    "\n",
    "For what concerns the modeling of the corn yield, it is done essentially in a few steps. After looking at the simple plot corn yield versus time it appeared clear they have a strong trend. The first assumption here is that the trend does not depend on the weather. This is reasonable given the huge growth of corn yield cannot be justified with weather. Thus, as first thing I estimated the trend for each county.\n",
    "\n",
    "My idea (motivated by the fact that such feature is quite common in time series) is that this growth is due to investments and technology. More corn yield leads to more investments and more than a linear trend is expected. Thus the **first transformation** that I have applied to the data is a **logarithmic** one. After the log transform there is still some trend that can has also been removed. However, to **remove the additional trend** I only took the data compatible with the weather data I had at my disposal. I have done this because forecast models work well with stationary data. Thus I wanted stationary data for the available dataset (i.e. weather dataset after 2000s) I could use to train my models.\n",
    "\n",
    "After the computation of the aggregated features (max number of days without rain, number of hot (T>35) days etc...), I have performed **correlation** plots between corn yield and each of these features for every month. These plots revealed something extremely useful. The correlation, for each feature, is a cyclic function of time (it is obvious that correlation in December must be similar to the one in January but it is still a good safety check) and the **peak of correlation** (or anti-correlation) for almost all features is reached during **summer between May and August**. This is very clear for the plot obtained including the whole weather dataset (i.e. the data are not filtered per state or county) but it is also clear in the individual plots of single states (except Minnesota whose plot looks very noisy).\n",
    "\n",
    "The correlation plot shows that precipitation, soil water content and soil water content standard deviation are positively correlated with corn yield during summer, while extreme temperatures (very hot days or maximum temperatures) are negatively correlated with corn yield during summer.\n",
    "\n",
    "Unfortunately this plot tells us something important: the crucial weather data to have good forecasts are between May and August. This means that with our dataset we cannot do reliable 2024 corn yield prediction as our 2024 weather data stop at the end of April. To include this information in our data forecast we should heavily increase the error bars (or variance or confidence range) compared to the simple confidence interval we have from our modeling. In this assignment, this is not included thus we should be less confident in our forecast compared to what data tell us. One could include this assessment by training the model with the full year weather for past years and then to do the same with just the data we have to estimate our 2024 corn yield, that is data until April. Then one could compute how much is the difference for example compared to real values of 2021, 2022 and 2023 between the two models and the real corn yield value. I did not do it myself but it should be doable with this notebook modifying the parameters in the first cell. I have only done the prediction for **2023** and it looks **very good (error of prediction of 1%)** when full year is included, especially with the random forest model. This can be done putting YEAR_PREDICTION to 2023 and month range (5,8) -it is enough-.\n",
    "\n",
    "To actualy show that the model is betteer than a simple fit, next to the prediction for 2024 there is the prediction for the year **2012**. I chose this year since it goes outsite the linear fit and it has a strong drop compared to 2011 and 2013. The random forest model is actually capable of predicting the drop as you can see from the picture. For this plot I did not retrain the model so the model has seen partially the data during trainig, but the match is nonetheless very good and it stresses that the random forrest is doing a very good job.\n",
    "\n",
    "Let us go to the more quantitative part:\n",
    "\n",
    "The models developed in this notebook start from data of each single county. The idea is to **model the corn yield of each county for 2024 and then to weight them** (the weight is the relative harvested area with respect to the total) for the final national corn yield prediction. Indeed from the plots drawn below it is clear that there are counties with very different areas (making some almost negligible for national corn yield).\n",
    "\n",
    "Here there is again some space for model improvement. Indeed during the training of our models all counties are treated in the same way, one could think of a customized loss function where more importance is given to data belonging to counties with large harvested areas.\n",
    "\n",
    "Further improvement could also be achieved with a model for the harvested areas. In this assignment I focused only on the yield, but one could apply similar modeling (using other datasets - not weather) for the harvested areas. Not all areas are available for 2024 thus we have to make some choice. There are **two possibilities** in the notebook for what concerns the **area** to use for the weight of the forecast:\n",
    "\n",
    "1) The last available area for the counties\n",
    "\n",
    "2) The average of the data after the year 2000 (to be coherent with the rest of the modeling)\n",
    "\n",
    "***Thus learning process happens on this transformed dataset and then the predicted values are re-transformed*** (in the opposite order) to go back to real values. In the evaluation of the uncertainty parameters I did not include also the uncertainty of the trend, but for a correct modeling this should be done.\n",
    "\n",
    "In this assignment I mostly focused on the modeling part, not on the efficiency of the models nor on the exploitation of the models. I am sure that given the two models here one could play with additional aggregated weather features and with some other parameters (i.e. tuning harvested areas, including other counties, changing parameters of the models and so on) to obtain more precise forecasts.\n",
    "\n",
    "The models used to give a quantitative estimation for 2024 are two:\n",
    "\n",
    "1) **Random Forest** (using scikit-learn)\n",
    "\n",
    "2) **Multi Layer Perceptron** (Neural Network with Pytorch)\n",
    "\n",
    "They are both very simple, and yet they give reasonable forecasts. Both of them could be further tuned even though I did not spend time trying to optimize the results.\n",
    "\n",
    "As a first step to improve, the random forrest could take as input the number of trees depending by the number of input features (now it is fixed thus it is not optimal for the case with monthly aggregated data)\n",
    "\n",
    "Since we are dealing with time series, another possible extension of this work could be the possibility of using different NN architectures that are more appropriate to model time series such as LSTM or the time-series version of Transformer. The latter could also give information about which hidden parameters are most critical thanks to the \"attention\" mechanism.\n",
    "\n",
    "I have also started to implement the probabilistic multilayer perceptron, and this is something I would finish in the future - it does not differ much from the simple MLP.\n",
    "\n",
    "Another simple option would be to perform a Bayesian regression, however it could be demanding with many dimensions (i.e. all weather features) thus I disregarded it at first.\n",
    "\n",
    "The notebook is logically divided in three parts:\n",
    "\n",
    "1) **Data engineering**\\\n",
    "In this first part weather data are loaded, the corn data are downloaded with an API (be sure the internet connection is fine and the API_KEY works), and there is some data manipulation for ordering correctly the data for the visualization and some checks between API downloaded data and manually downloaded data - the part is commented out since you need the file to load but you can still see the code.\n",
    "\n",
    "2) **Data exploration**\\\n",
    "In this part there is some exploration of data done plotting several quantities to understand where and how to start the modeling. Some parameters as temperature, precipitation and soil moisture are plotted for states with very different corn yield. The goal was to see if already visually when comparing different locations with very different yield simple plots can already be inspiring. From those, one can target better subsequent analysis.\\\n",
    "After, there is the calculation of weather aggregated quantities. In particular I have computed: tmax_mean, tmax_min, tmax_max, tmin_mean, tmin_max, tmin_min, tmin_std, precip_max, precip_std, precip_mean, swvl2_min, swvl2_mean, swvl2_std, hot_day (days where temperature is above 35 degrees), cold_day (days where temperature is below -5), max_cons_dry_days (maximum consecutive number of days without rain).\n",
    "\n",
    "3) **Modelling and prediction**\\\n",
    "In this part there is the modeling for the training algorithms. There is first the random forest, with training and prediction. Then there is the Neural Network with the training and prediction.\n",
    "The random forest is trained twice and it generates two predictions. The first includes only the yearly aggregated features (but including only the months between January and April) and the second one includes monthly aggregated features (thus 4 times more data than the yearly aggregated case). The Neural Network is trained only on the monthly aggregated data. Both the models give very similar results, with a slight overestimation of the 2023 value (when the input YEAR_PREDICTION is set to 2023) of about 2%. However, training the random forest with the relevant weather dataset for July (i.e. taking weather data from May to August), the model perfectly finds the right 2023 value.\n",
    "The confidence level in both models is about 8% of the mean value, i.e. data are supposed to fall inside 195 (mean value predicted 2024) Â±10/15. The predicted value is very similar for both the random forest and the NN.\n",
    "The NN has been trained with the data of the whole nation together, and it could be better tuned if it was trained state-by-state. Indeed with the random forest I show that the mean root square error is 0.15 when the whole data are trained together and 0.13 when it is trained state-by-state. I expect a similar improvement to occur also for the NN.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are input parameters that can be changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR_PREDICTION      = 2024  # year for which we want to do the forecast\n",
    "                             # The training (and testing) dataset do include until   \"YEAR_PREDICTION\"\n",
    "                             # if YEAR_PREDICTION = 2020 only data until 2019 are used for training\n",
    "\n",
    "MONTHS_RANGE         = (1,4) # This is the interval of the year -espressed in month [begging_month, final_month]\n",
    "                             # that we use to train the model and predict the results.\n",
    "                             # It is (1,4) because weather data for 2024 only arrive until April\n",
    "\n",
    "features_NN_training = \"all\" # This uses all possible features for the training of the NN, \n",
    "                             # the full list is listed below. As next step I would surely allow the user to \n",
    "                             # specify only a subset of it. I tried to keep the code generic enough to do it, but I have never tested\n",
    "                             # if a subset of feature runs top-bottom.\n",
    "\n",
    "AREA_AVERAGED        = False  # This parameter is to define what to do with the harvested areas.\n",
    "                              # True  --> the area is the averaged (within a county) area over the last 20 years.\n",
    "                              # False --> the area is the most recent available area (for each county).\n",
    "\n",
    "                              # Indeed while there is clear an increasing trend for corn yield the harvested area is more noisi\n",
    "\n",
    "\n",
    "API_KEY              = \"\" # Insert here the API key to connect to the USA government website and download the data. \n",
    "                                                              \n",
    "\n",
    "\n",
    "RANDOM_FOREST = False\n",
    "\n",
    "NN_MLP_STATE  = False # logical not yet implemented\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "to select only few use e.g. features_NN_training = ['tmax_mean', 'tmax_min', 'tmax_max']\n",
    "['tmax_mean', 'tmax_min', 'tmax_max','tmin_mean', \n",
    " 'tmin_max', 'tmin_min', 'tmin_std', 'precip_max',\n",
    " 'precip_std', 'precip_mean', 'swvl2_min', 'swvl2_mean', 'swvl2_std',\n",
    "       'hot_day', 'cold_day', 'max_cons_dry_days']\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "wheater_df = pd.read_parquet(\"hist_wx_df.parquet\");\n",
    "\n",
    "wheater_df = wheater_df.sort_values(by=[\"adm1_name\",\"adm2_name\",\"date\"]);\n",
    "wheater_df.head(1)\n",
    "\n",
    "states   = wheater_df['adm1_name'].unique().tolist()      # Unique state names\n",
    "places = dict()\n",
    "for state in states:\n",
    "    tmp = wheater_df[wheater_df[\"adm1_name\"]==state]['adm2_name'].unique().tolist()\n",
    "    places[state] = tmp\n",
    "    \n",
    "places.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is crucial to see which data we have available for a prediction of 2024. \n",
    "We have until the **last day** in hist_wx_df dataframe, that is we have to exploit weather data until the end of April.\n",
    "\n",
    "We still use all our data available because it can be useful to see how much data only available until end of April under-perform compared to full data (until harvest time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_group(df):\n",
    "    df = df.set_index('date').asfreq('D')\n",
    "    df[['adm1_name', 'adm2_name']] = df[['adm1_name', 'adm2_name']].ffill().bfill()\n",
    "    numeric_cols = df.select_dtypes(include='number').columns\n",
    "    df[numeric_cols] = df[numeric_cols].interpolate()\n",
    "    return df\n",
    "\n",
    "wx_filled      = wheater_df.groupby(['adm1_name', 'adm2_name'], group_keys=False).apply(interpolate_group).reset_index()\n",
    "wx_filled[\"year\"]    = wx_filled[\"date\"].dt.year\n",
    "wx_filled[\"month\"]   = wx_filled[\"date\"].dt.to_period(\"M\")\n",
    "wx_filled['month_N'] = wx_filled['date'].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to check that all weather data belong to the same period, i.e. the first and the last day is always the same accross all counties. \n",
    "\n",
    "To check this, we create a **dictionary** with keys corresponding to the first and last available day for the weather. Inside each key we have a list of all counties that have those days weather data. The format is **\"county-state\"** (e.g. ) ['Will-Illinois', 'Williamson-Illinois', ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "time_window = [defaultdict(list), defaultdict(list)]\n",
    "\n",
    "grouped = wx_filled.groupby(['adm1_name', 'adm2_name'])['date']\n",
    "for state in places.keys():\n",
    "    for county in places[state]:\n",
    "        dates = grouped.get_group((state, county)).sort_values()\n",
    "        first_day = dates.iloc[0]\n",
    "        last_day = dates.iloc[-1]\n",
    "\n",
    "        time_window[0][first_day].append(f\"{county}-{state}\")\n",
    "        time_window[1][last_day].append(f\"{county}-{state}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time_window is a a list of 2 dictionary. \n",
    "\n",
    "The key of the first dictionary <time_window[0]> is the initial date available for every city and inside the key there is the list of couties with that initial date\n",
    "\n",
    "The key of the second dictionary <time_window[1]> is the final date available for every city and inside the key there is the list of couties with that final date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the output of the dictionaries to spot anomalies (We can see that **'Monroe-Kentucky'** has a shorter time window database for the weather, first day available being **2011-01-01**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in time_window[0]:\n",
    "    print(k, time_window[0][k])\n",
    "print(\"---------------------------\")\n",
    "for k in time_window[1]:\n",
    "    print(k, time_window[1][k])\n",
    "\n",
    "oldest_time = min(time_window[0].keys())\n",
    "newest_time = max(time_window[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than downloading manually all the files we need we can download them using the **API_KEY** provided by the USA government."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def download_corn_data(state=\"none\", county=\"none\",Area_harvsted=False):\n",
    "\n",
    "    if state == \"none\":\n",
    "        level = \"NATIONAL\"\n",
    "    else:\n",
    "        level = \"COUNTY\"\n",
    "    \n",
    "\n",
    "    params = {\n",
    "        \"key\": API_KEY,\n",
    "        \"source_desc\": \"SURVEY\",\n",
    "        \"commodity_desc\": \"CORN\",\n",
    "        \"statisticcat_desc\": \"YIELD\",\n",
    "        \"unit_desc\": \"BU / ACRE\",\n",
    "        \"agg_level_desc\": level,\n",
    "        \"freq_desc\": \"ANNUAL\",\n",
    "        \"reference_period_desc\": \"YEAR\",      \n",
    "        \"year__GE\": 1830,\n",
    "        \"year__LE\": 2024,\n",
    "        \"format\": \"CSV\"\n",
    "    }\n",
    "\n",
    "    if Area_harvsted:\n",
    "        params[\"statisticcat_desc\"] = \"AREA HARVESTED\"\n",
    "        params[\"unit_desc\"] = \"ACRES\"\n",
    "        params[\"util_practice_desc\"] = \"GRAIN\"  # <-- aggiungere anche qui\n",
    "               \n",
    "    if not state == \"none\":\n",
    "        params[\"state_name\"] = state.upper()\n",
    "    if not county == \"none\":\n",
    "        params[\"county_name\"] = county.upper()\n",
    "\n",
    "    url = \"https://quickstats.nass.usda.gov/api/api_GET/\"\n",
    "    r = requests.get(url, params=params)\n",
    "\n",
    "    with open(\"corn_yield_1950_2024.csv\", \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "    df = pd.read_csv(\"corn_yield_1950_2024.csv\")\n",
    "    df.rename(columns={\"county_name\": \"adm2_name\", \"state_name\": \"adm1_name\"}, inplace=True)\n",
    "    try:\n",
    "        df[\"adm2_name\"] = df[\"adm2_name\"].str.title()\n",
    "        df[\"adm1_name\"]  = df[\"adm1_name\"].str.title()\n",
    "    except:\n",
    "        print(\"using national data no state_name or county_name\")\n",
    "\n",
    "    if Area_harvsted:\n",
    "        df.rename(columns={\"Value\": \"Area\"}, inplace=True)\n",
    "        df[\"Area\"] = pd.to_numeric(df[\"Area\"].str.replace(\",\", \"\"), errors=\"coerce\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to create a function that allows to perform plot on quantities on random or specific counties. We will use it for explorative plot analisysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def random_plot(df_list, y_axis=[\"Value\"], year_filter=2000, marker=\"\",location=\"rand\",ax=None,Verbose=False):\n",
    "    if not isinstance(df_list, list):\n",
    "        df_list = [df_list]\n",
    "    if not isinstance(y_axis, list):\n",
    "        y_axis = [y_axis] * len(df_list)\n",
    "    if not isinstance(marker, list):\n",
    "        marker = [marker] * len(df_list)\n",
    "    elif len(marker) < len(df_list):\n",
    "        marker = [marker[0]] * len(df_list)\n",
    "        print(\"list marker too short compared to number of dataset - set first value of marker for all df\")\n",
    "\n",
    "    df = df_list[0]\n",
    "    state_loc_list    = df[\"adm1_name\"].unique()\n",
    "    \n",
    "    if location == \"rand\": \n",
    "        while (True):\n",
    "            N_LOC_RANDOM      = random.randint(0,len(state_loc_list)-1)\n",
    "            state             = state_loc_list[N_LOC_RANDOM]\n",
    "            counties_loc      = df[df[\"adm1_name\"]==state][\"adm2_name\"].unique()\n",
    "            county            = counties_loc[random.randint(0,len(counties_loc)-1)]\n",
    "            if not(\"Other (Combined)\" in county):\n",
    "                break\n",
    "    else:\n",
    "        state  = location[0]\n",
    "        county = location[1]\n",
    "\n",
    "\n",
    "    if ax==None:\n",
    "        fig, ax = plt.subplots(ncols=len(y_axis),nrows=1,figsize=(6,2))\n",
    "\n",
    "    if len(y_axis) > 1:\n",
    "        ax=ax.flatten()\n",
    "    g_ax = ax\n",
    "\n",
    "    for i_df, df in enumerate(df_list):\n",
    "        mask = (df[\"adm1_name\"] == state) & (df[\"adm2_name\"] == county ) &  (df[\"year\"] >= year_filter ) \n",
    "\n",
    "        for idx, val_plot in enumerate(y_axis):\n",
    "            if len(y_axis)>1:\n",
    "                g_ax = ax[idx]\n",
    "\n",
    "            df[mask].plot(x=\"year\",y=val_plot,ax=g_ax,marker=marker[i_df])\n",
    "    \n",
    "\n",
    "    if Verbose:\n",
    "        print(county, state, year_filter)\n",
    "    return (state,county)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be positive about the right way to call the API we plot the data from the manually downloaded file versus those we obtain we API. We try with national and Indiana state data.\n",
    "( I have done the test on my CV but here to run you would need the local .csv file thus I have commented this part of the code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "corn_indiana  = pd.read_csv(\"C4A8B611-247C-3C5C-8F5E-36956CBC3450-Indiana.csv\")\n",
    "corn_indiana.rename(columns={\"County\": \"adm2_name\", \"State\": \"adm1_name\", \"Year\":\"year\"}, inplace=True)\n",
    "\n",
    "df            = download_corn_data(\"Indiana\")\n",
    "df            = df.sort_values(by=[\"adm2_name\",\"year\"])\n",
    "corn_indiana  = corn_indiana.sort_values(by=[\"adm2_name\",\"year\"])\n",
    "\n",
    "corn_indiana[\"adm2_name\"] = corn_indiana[\"adm2_name\"].str.title()\n",
    "corn_indiana[\"adm1_name\"]  = corn_indiana[\"adm1_name\"].str.title()\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2,nrows=1,figsize=(6,2))\n",
    "random_plot([df, corn_indiana], marker=[\"*\",\"\"],  location=(\"Indiana\", \"Benton\"),ax=ax[0])\n",
    "random_plot([df, corn_indiana], marker=[\"*\",\"\"],year_filter=0, location=(\"Indiana\", \"Benton\"),ax=ax[1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block is used to download the corn data, it aggregates harvested area with corn yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corn_value_tot = []\n",
    "for state in states:\n",
    "    print(state)\n",
    "    corn_df = download_corn_data(state)\n",
    "    area_df = download_corn_data(state=state ,Area_harvsted=True)\n",
    "    \n",
    "    mask = (\n",
    "        corn_df[\"prodn_practice_desc\"].str.contains(\"ALL PRODUCTION PRACTICES\", na=False) &\n",
    "        ~corn_df[\"adm2_name\"].str.contains(r\"Other.*Counties\", na=False, regex=True)\n",
    "    )\n",
    "    mask_area = (\n",
    "        area_df[\"prodn_practice_desc\"].str.contains(\"ALL PRODUCTION PRACTICES\", na=False) &\n",
    "        ~area_df[\"adm2_name\"].str.contains(r\"Other.*Counties\", na=False, regex=True)\n",
    "    )\n",
    "\n",
    "    corn_value = corn_df.loc[mask,      ['adm2_name', 'adm1_name', 'year', 'Value']]\n",
    "    area_value = area_df.loc[mask_area, ['adm2_name', 'adm1_name', 'year', 'Area']]\n",
    "\n",
    "    # corn_value_merged = corn_value.copy()\n",
    "    corn_value_merged = corn_value.merge(area_value, on=['adm2_name', 'adm1_name','year'],how=\"left\")\n",
    "    \n",
    "    if len(corn_value_tot) == 0:\n",
    "        corn_value_tot = corn_value_merged.copy()\n",
    "    else:\n",
    "        corn_value_tot = pd.concat([corn_value_tot, corn_value_merged], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block is used to extract the harvested area that will be used to weight the single county yield in order to get the national one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if AREA_AVERAGED:\n",
    "    mask = corn_value_tot[\"year\"].between(2000, 3000)\n",
    "    area_df_pred  = corn_value_tot[mask].groupby(['adm2_name', 'adm1_name']).agg({'Area': ['mean', 'std']})\n",
    "    new_columns = ['_'.join(col) for col in agg.columns]\n",
    "    area_df_pred.columns = new_columns\n",
    "    area_df_pred.reset_index(inplace=True)\n",
    "    area_df_pred.rename(columns={\"Area_mean\": \"Area_prediction\"}, inplace=True)\n",
    "else:\n",
    "    idx = corn_value_tot.groupby(['adm2_name', 'adm1_name'])['year'].idxmax()\n",
    "    area_df_pred = corn_value_tot.loc[idx, ['adm2_name', 'adm1_name', 'Area']].reset_index(drop=True)\n",
    "    area_df_pred.rename(columns={\"Area\": \"Area_prediction\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to explore da corn production data to see how they look and if there is some immediate feature we can see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,  ax      = plt.subplots(ncols=4,nrows=1,figsize=(15,3))\n",
    "fig2, ax_area = plt.subplots(ncols=4,nrows=1,figsize=(15,3))\n",
    "\n",
    "for idx in range(len(ax)):\n",
    "    loc1 = random_plot(corn_value_tot,year_filter=1900,ax=ax[idx])\n",
    "    loc2 = random_plot(corn_value_tot,year_filter=1900,ax=ax[idx])\n",
    "    ax[idx].legend([loc1, loc2])\n",
    "\n",
    "    random_plot(corn_value_tot,y_axis=\"Area\",year_filter=1900,ax=ax_area[idx])\n",
    "    random_plot(corn_value_tot,y_axis=\"Area\",year_filter=2010,ax=ax_area[idx])\n",
    "    ax_area[idx].legend([loc1, loc2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plots above we can see that in many couties the corn yield has been growing in time since after 1940. Thus we include this trend in our modeling.\n",
    "\n",
    "In particular we will remove the trend since it cannot depend on weather condition (as they are mostly cyclic) and cannot alone explain the strong trend but rather the fluctuations.\n",
    "\n",
    "Thus, after de-trending we can try to model how weather does affect yearly fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before modeling we want to see how data are correlated and a first insight is given by the standard deviation that we plot inside each state for every year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, nrows=1, figsize=(14, 4))\n",
    "grouped = corn_value_tot.groupby(\"adm1_name\")\n",
    "ii = -1\n",
    "for key_state, state_df in grouped:\n",
    "    ii = ii + 1\n",
    "    std_df  = state_df.groupby(\"year\", as_index=False)[\"Value\"].std()\n",
    "    mean_df = state_df.groupby(\"year\", as_index=False)[\"Value\"].mean()\n",
    "    mean_state = mean_df[\"Value\"].to_numpy()\n",
    "    year = std_df[\"year\"].to_numpy()\n",
    "    std_state = std_df[\"Value\"].to_numpy()\n",
    "    if ii < len(grouped)/2:\n",
    "        ax[0].plot(year, std_state,label=key_state)    \n",
    "        ax[1].plot(year, mean_state,label=key_state)\n",
    "    else:\n",
    "        ax[2].plot(year, std_state,label=key_state)    \n",
    "        ax[3].plot(year, mean_state,label=key_state)\n",
    "for ii in range(4):\n",
    "    ax[ii].legend()\n",
    "    if np.mod(ii,2)==0:\n",
    "        ax[ii].set_title(\"std\")\n",
    "    else:\n",
    "        ax[ii].set_title(\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the standard deviations are growing with time and also their time-fluctuations increase. \n",
    "\n",
    "The fact that also std has a trend may suggest that std is not only a function of weather -that shoud have no such a strong trend- but rather depends on local (county) administration and local technology.\n",
    "\n",
    "It thus seems reasonable and better for our pourposes to detrend data county by county to eliminate the effects due to non-random factors.\n",
    "\n",
    "It is furthermore important to note that states with higher std fluctuation are not necessarely those with larger mean. In particular it appears Kansas and South Dakota have huge fuctuations compared to Iowa and Ohio (we split in two groups the plot for a better readibility of the plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with a visual analysis of how  weather can impact on production. For a plot to carry meanigfull information we must compare states with very different std and mean corn yield. Thus, according to the previous plot we choose **Ohio** and **South Dakota**. We plot as function of time (limited to the years where there is a big difference in std and mean of corn yield), the daily mean (accross the state) of **Tmin**, the daily mean (accross the state) of the **precipitations** and the daily mean (accross the state) of the **swvl1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df = wx_filled.groupby([\"adm1_name\", \"date\"]).agg({\"tmin\": [\"std\", \"mean\"],\"precip\": [\"mean\", \"std\"], \"swvl1\":[\"mean\",\"std\"],\"swvl2\":[\"mean\",\"std\"]}).reset_index()\n",
    "fig, ax = plt.subplots(ncols=2,nrows=4,figsize=(18,8.5))\n",
    "ax=ax.flatten()\n",
    "plt.subplots_adjust(hspace=0.8, wspace=0.2)\n",
    "\n",
    "for key_state, state_df in daily_df.groupby([\"adm1_name\"]):\n",
    "    if key_state[0] in [\"Ohio\",\"South Dakota\"]:\n",
    "        mask       = (state_df[\"date\"] >= \"2000-01-01\") & (state_df[\"date\"] <= \"2006-12-31\")\n",
    "        state_df[mask].plot(x=\"date\",y=(\"tmin\",\"mean\"),label=key_state[0],ax=ax[0])\n",
    "        state_df[mask].plot(x=\"date\",y=(\"tmin\",\"std\"),label=key_state[0],ax=ax[1])\n",
    "        state_df[mask].plot(x=\"date\",y=(\"precip\",\"mean\"),label=key_state[0],ax=ax[2])\n",
    "        state_df[mask].plot(x=\"date\",y=(\"precip\",\"std\"),label=key_state[0],ax=ax[3])\n",
    "        state_df[mask].plot(x=\"date\",y=(\"swvl1\",\"mean\"),label=key_state[0],ax=ax[4])\n",
    "        state_df[mask].plot(x=\"date\",y=(\"swvl1\",\"std\"),label=key_state[0],ax=ax[5])\n",
    "        state_df[mask].plot(x=\"date\",y=(\"swvl2\",\"mean\"),label=key_state[0],ax=ax[6])\n",
    "        state_df[mask].plot(x=\"date\",y=(\"swvl2\",\"std\"),label=key_state[0],ax=ax[7])\n",
    "\n",
    "ax[0].set_title(\"Mean tmin\")\n",
    "ax[1].set_title(\"std tmin\")\n",
    "ax[2].set_title(\"mean precip\")\n",
    "ax[3].set_title(\"std pricip\")\n",
    "ax[4].set_title(\"mean swvl1\")\n",
    "ax[5].set_title(\"std swvl1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are pretty clear showing large differences between Ohio and South Dakota for all parameters. One important parameter is it swvl -**soil water content**-. First we see that swv1 and swvl2 are stronlgy correlated, thus in a first approximation we skip the analysis of swvl2. **South Dakota** has a huge std compared to Ohio, and perhaps this may explain the huge std in the corn production.\n",
    "\n",
    "We look now to a cleaner dataset obtained averaging as before but also accross every single month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly = wx_filled.groupby([\"adm1_name\", \"month\"]).agg({\"tmin\": [\"mean\", \"min\"],\"tmax\": [\"mean\", \"max\"]}).reset_index()\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2,nrows=2,figsize=(18,5.8))\n",
    "ax=ax.flatten()\n",
    "plt.subplots_adjust(hspace=0.8, wspace=0.2)\n",
    "\n",
    "for key_state, state_df in monthly.groupby([\"adm1_name\"]):\n",
    "    if key_state[0] in [\"Ohio\",\"South Dakota\"]:\n",
    "        mask       = (state_df[\"month\"] >= \"2000-01\") & (state_df[\"month\"] <= \"2006-12\")\n",
    "        state_df[mask].plot(x=\"month\",y=(\"tmin\",\"mean\"),label=key_state[0],ax=ax[0])\n",
    "        state_df[mask].plot(x=\"month\",y=(\"tmin\",\"min\"),label=key_state[0],ax=ax[1])\n",
    "        state_df[mask].plot(x=\"month\",y=(\"tmax\",\"mean\"),label=key_state[0],ax=ax[2])\n",
    "        state_df[mask].plot(x=\"month\",y=(\"tmax\",\"max\"),label=key_state[0],ax=ax[3])\n",
    "\n",
    "ax[0].set_title(\"Mean tmin\")\n",
    "ax[1].set_title(\"min tmin\")\n",
    "ax[2].set_title(\"mean tmax\")\n",
    "ax[3].set_title(\"max tmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are very clear: in particular the extremes of temperatures are very different and more extreme for South Dakota. Temperature minima values (thus the minimum temperature for each month) of South Dakota are always lower than those of Ohio. Similarly, during summer maxima values of temperatures (thus the maximum temperature for each month) of South Dakota are always higher than those of Ohio. These huge thermal differences might be one of the reasons behind the volatility of South Dakota corn yield."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a meaninfull analysis raw datas are not enough, here we build aggregate data that could be usefull for corn yield modeling.\n",
    "We construct a Dataset over a period  (moths or years). \\\n",
    "**The dataset includes:** \\\n",
    "Tmax mean, Tmax max, Tmin mean, Tmin min, total precip (sum), mean of precip, mean and sum of swvl1, maximum consecutive days without rain, number of hot (T>35) and cold (T<-10) days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featuring_engineering(months_range, period, state):\n",
    "\n",
    "    wx_season = wx_filled[wx_filled['month_N'].between(months_range[0], months_range[1])].copy()\n",
    "    if not(state[0] == \"all\"):\n",
    "        wx_season = wx_season[wx_season['adm1_name'].isin(state)]\n",
    "\n",
    "\n",
    "    #  Aggregate weather features\n",
    "    agg         =   wx_season.groupby(['adm2_name', 'adm1_name', period]).agg({'tmax': ['mean','min', 'max'],\n",
    "                                                                               'tmin': ['mean', 'max', 'min','std'],'precip': ['max','std','mean'],\n",
    "                                                                               \"swvl2\":['min','mean','std']}) \n",
    "    agg.columns = ['_'.join(col) for col in agg.columns] # To avoind tuples and sub-coloumns\n",
    "    agg         = agg.reset_index()\n",
    "\n",
    "    #  Add hot day count\n",
    "    wx_season.loc[:, 'hot_day'] = wx_season['tmax'] > 35\n",
    "    hot_days = wx_season.groupby(['adm2_name', 'adm1_name', period])['hot_day'].sum().reset_index()\n",
    "    agg      = agg.merge(hot_days, on=['adm2_name', 'adm1_name', period], how='left')\n",
    "\n",
    "    #  Add cold day count\n",
    "    wx_season.loc[:, 'cold_day'] = wx_season['tmin'] < -4 \n",
    "    cold_days = wx_season.groupby(['adm2_name', 'adm1_name', period])['cold_day'].sum().reset_index()\n",
    "    agg      = agg.merge(cold_days, on=['adm2_name', 'adm1_name', period], how='left')\n",
    "\n",
    "    dry_records = []\n",
    "    for id_group, group in wx_season.groupby(['adm2_name', 'adm1_name', period]):\n",
    "        dry = group['precip'] == 0\n",
    "        max_cons_days_dry    = dry.astype(int).groupby((~dry).cumsum()).sum().max()\n",
    "        dry_records.append((*id_group, max_cons_days_dry))\n",
    "\n",
    "    dry_df = pd.DataFrame(dry_records, columns=['adm2_name', 'adm1_name', period, 'max_cons_dry_days'])\n",
    "    agg = agg.merge(dry_df, on=['adm2_name', 'adm1_name', period], how='left')\n",
    "\n",
    "    if period == \"month\":\n",
    "        agg[\"year\"]    = agg[\"month\"].dt.year\n",
    "        agg['month_N'] = agg['month'].dt.month\n",
    "\n",
    "\n",
    "    return agg\n",
    "\n",
    "def extract_feature_list(df):\n",
    "    features_wanted = [c for c in df.columns if c not in ['adm2_name', 'adm1_name','Value','month','year']]\n",
    "    return features_wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build a function for the detrending of corn yield. We use statsmodels.\n",
    "\n",
    "A priori one should find the good rule of the trend, that might not be the same for every county and most importantly it will be likely different for every state. However, to keep it simple here we perform the most common detrending techniques.\n",
    "We start first we a logarithmic transform and then we apply a simple regression with statsmodels. We could also normalize the data around the mean and std 1, but it turned out to be not always necessary, it depends on the model.\\\n",
    "Thus the function has three boolean inputs: log_detrend, trend_detrend, normalize_data_mean_std. If activated, the transformation order is: #1 log, #2 regression, #3 mean-std normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "def detrending_fun_statmod(log_detrend, trend_detrend, normalize_data_mean_std):\n",
    "    not_included_groups = []\n",
    "    model_trend_dict = {}\n",
    "    for (adm1, adm2), group in corn_value_tot.groupby([\"adm1_name\", \"adm2_name\"]):\n",
    "        if not(adm2 in places[adm1]):\n",
    "            continue\n",
    "        group = group[group[\"year\"] >= 2000]\n",
    "        y = group[\"Value\"].to_numpy()\n",
    "        \n",
    "        if np.shape(y)[0]<2:\n",
    "            corn_value_tot.loc[group.index, \"Value-detrand\"] = np.nan\n",
    "            not_included_groups.append([adm1, adm2])\n",
    "            continue     \n",
    "\n",
    "        if log_detrend:\n",
    "          y = np.log(y+1)\n",
    "\n",
    "        # x=x.reshape(-1, 1)\n",
    "        trend     = 0\n",
    "        residual  = y  \n",
    "\n",
    "        if trend_detrend:\n",
    "          x = sm.add_constant(group[\"year\"].to_numpy())\n",
    "          model = sm.OLS(y, x).fit()\n",
    "          trend    =  model.predict(x)\n",
    "          residual = y - trend\n",
    "\n",
    "        mean      = y.mean()\n",
    "        std       = y.std()     \n",
    "\n",
    "        if normalize_data_mean_std:\n",
    "          residual = (y - mean) / std       \n",
    "        \n",
    "        corn_value_tot.loc[group.index, \"Value-detrand\"] = residual\n",
    "        # corn_value_tot.loc[group.index, \"trend\"] = trend\n",
    "        corn_value_tot.loc[group.index, \"std\"] = std\n",
    "        corn_value_tot.loc[group.index, \"mean\"] = mean\n",
    "        model_trend_dict[(adm1, adm2)] = model\n",
    "\n",
    "    return not_included_groups, model_trend_dict\n",
    "\n",
    "def restore_trend(df, model_trend_dict, log_detrend, trend_detrend, normalize_data_mean_std):\n",
    "    restored_values = []    \n",
    "    for (adm1, adm2), group in df.groupby([\"adm1_name\", \"adm2_name\"]):        \n",
    "        group = group[group[\"year\"] >= 2000].copy()\n",
    "        y = group[\"Value_predicted\"].to_numpy()\n",
    "        if len(y) == 0:\n",
    "            print(\"length is 0\")\n",
    "            continue\n",
    "\n",
    "        if normalize_data_mean_std:\n",
    "            std  = group[\"std\"].to_numpy()\n",
    "            mean = group[\"mean\"].to_numpy()\n",
    "            y    = y * std + mean\n",
    "\n",
    "        if trend_detrend:\n",
    "            model = model_trend_dict.get((adm1, adm2), None)\n",
    "            if model is None:\n",
    "                continue  \n",
    "\n",
    "            x_raw   = group[\"year\"].to_numpy().reshape(-1, 1) # needed for compativility with the model in case of single x-y for the group\n",
    "            x_years = sm.add_constant(x_raw, has_constant='add') # needed for compativility with the model in case of single x-y for the group\n",
    "            trend   = model.predict(x_years)\n",
    "            y       = y + trend\n",
    "\n",
    "        if log_detrend:\n",
    "            y = np.exp(y) - 1\n",
    "\n",
    "        restored_values.append(pd.DataFrame({\"index\": group.index,\"Value_restored\": y}))\n",
    "\n",
    "    restored_df = pd.concat(restored_values).set_index(\"index\")\n",
    "    df.loc[restored_df.index, \"Value_restored\"] = restored_df[\"Value_restored\"]\n",
    "\n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset has some holes and it is not always possible to handle tranformation (for example detranding), we create a funtion that removes the NaN indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_nan_dataset(df,feature=\"Value\"):\n",
    "    \n",
    "    df[feature].isna().any()\n",
    "    mask_nonan = ~df[feature].isna()\n",
    "\n",
    "    no_nan_dataset = df[mask_nonan]\n",
    "    if no_nan_dataset[feature].isna().any():\n",
    "        print(\"there are still bad data with NaN values\")\n",
    "\n",
    "    return no_nan_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now before any prediction model we want to remove the trends for data.. We compute county-by-county trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transf     = True\n",
    "linear_detrand = True\n",
    "mean_std_norm  = False\n",
    "\n",
    "corn_value_tot                     = exclude_nan_dataset(corn_value_tot,feature=\"Value\")\n",
    "\n",
    "excluded_dataset, model_trend_dict = detrending_fun_statmod(log_detrend=log_transf, trend_detrend=linear_detrand, normalize_data_mean_std=mean_std_norm)\n",
    "\n",
    "corn_value_tot                     = exclude_nan_dataset(corn_value_tot,feature=\"Value-detrand\")\n",
    "\n",
    "corn_value_tot                     = exclude_nan_dataset(corn_value_tot,feature=\"Area\")\n",
    "\n",
    "random_plot([corn_value_tot],y_axis=[\"Value\",\"Value-detrand\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediate function to build the features and aggegate with con Value and harvested area\n",
    "def featuring_training_dataset(months_range=(3,7),period=\"year\",state=[\"all\"]):\n",
    "    feat_year               = featuring_engineering(months_range, period, state)\n",
    "    # feat_year has the same cities available for every year, 960 (\"adm1_name\", \"adm2_name\") different\n",
    "    train_yield_feat_year  = feat_year.merge(corn_value_tot, on=['adm2_name', 'adm1_name', 'year'], how='left')\n",
    "    train_yield_feat_year  = train_yield_feat_year.merge(area_df_pred,   on=['adm2_name', 'adm1_name'],         how='left')\n",
    "\n",
    "    train_yield_feat_year  = exclude_nan_dataset(train_yield_feat_year,feature=\"Value-detrand\")\n",
    "\n",
    "    return feat_year, train_yield_feat_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to look for important parameters for the modeling. The simplest idea is see which are the most correlated parameters. Since correlation may change during the year, the plot is a function of the month of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_plot(df, features, label,ax=None):\n",
    "    partial = df[features]\n",
    "    corr_matrix = []\n",
    "    for ii in range(df[\"month_N\"].min(), df[\"month_N\"].max()):\n",
    "        mask = partial['month_N'] == ii\n",
    "        df_corr = partial[mask].corr()[label]\n",
    "        corr_values = [df_corr.get(f, np.nan) for f in features[:-1]]  # exclude target\n",
    "        corr_matrix.append(corr_values)\n",
    "\n",
    "    corr_matrix = np.array(corr_matrix)  # shape: (12, num_features)\n",
    "    if ax==None:\n",
    "        fig, ax = plt.subplots(figsize=(5, 3))  # note the 's' in 'subplots'\n",
    "\n",
    "    for i, var in enumerate(features[:-1]):\n",
    "        ax.plot(range(df[\"month_N\"].min(), df[\"month_N\"].max()), corr_matrix[:, i], label=var)\n",
    "\n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.set_ylabel(\"Correlation with Value-detrand\")\n",
    "    ax.legend(fontsize=6, loc='best', ncol=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the data, then we plot the correlation for the whole dataset and then we do the same for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell we build the dataset of feature, we keep it alone since it takes time to run\n",
    "feat_month, train_yield_feat_month = featuring_training_dataset(months_range=(1,12),period=\"month\")\n",
    "label    = \"Value-detrand\"\n",
    "features = ['tmin_min','precip_mean','hot_day', 'swvl2_mean','cold_day', 'max_cons_dry_days','swvl2_std','month_N','tmax_mean', 'tmax_max','tmin_mean']\n",
    "features.append(label)\n",
    "correlation_plot(train_yield_feat_month, features, label)\n",
    "feat_month_copy = feat_month.copy()\n",
    "train_yield_feat_month_copy = train_yield_feat_month.copy() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is clear during the several months the several features may change the way they affect the corn yield. In particular it is  clear that the crucial phase is around July (month #7), with highest values of correations (and anticorrelations).\n",
    "\n",
    "Higher precipitations and soil water content during summer seem to be beneficial for corn fields.\n",
    "\n",
    "Furthermore, extreme temperature are anticorrelated with corn yield.\n",
    "\n",
    "The positive correlation does not mean that incresing indefinetively the precipitation and soil water content during summer will always be beneficial (beyond a certain threshold they will start to be anticorrelated as dependence is probably non monotonic). The same argument is valid for negative correlated features. This trend only must be interpreted with respect to the average values in the dataset and it is true only for this dataset, it is not a universal feature.\n",
    "\n",
    "However, some important metric for forecasting can be extracted. This plot may help us to tune better our models. \n",
    "\n",
    "Finally, since our weather dataset for 2024 only arrive until April 2024, it is unlikely to perform a good prediction for 2024 as we indeed have seen that is June-July the period where the features are mostly correlated with the corn production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label    = \"Value-detrand\"\n",
    "features = ['tmin_min','precip_mean','hot_day', 'swvl2_mean','cold_day', 'max_cons_dry_days','swvl2_std','month_N','tmax_mean', 'tmax_max','tmin_mean']\n",
    "features.append(label)\n",
    "\n",
    "fig, ax = plt.subplots(ncols=3,nrows=5,figsize=(20,20))\n",
    "ax=ax.flatten()\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "id_ax = -1\n",
    "for group_idx, group in train_yield_feat_month.groupby(\"adm1_name\"):\n",
    "    id_ax += 1\n",
    "    correlation_plot(group, features, label,ax=ax[id_ax])\n",
    "    ax[id_ax].set_title(group_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state-by-state plot shows also similar trend to the one analized using the full database, but the trends are more noise (likely due to fewer datas)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to predict the corn yield with the help on a random forest (sklearn). In the following cell we build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def random_forest_model(df_training, features,plot_y_test=False):\n",
    "\n",
    "       X = df_training[features]\n",
    "       y = df_training[\"Value-detrand\"]\n",
    "\n",
    "       X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "       test_dataset  = df_training.loc[X_test.index,  [\"year\", \"adm2_name\", \"adm1_name\",\"std\",\"mean\",\"Area_prediction\",\"Value\"]]\n",
    "\n",
    "       # max_depth=7 has been found after a \"fine tuning\"\n",
    "       model =  RandomForestRegressor(n_estimators=200,  max_depth=7, bootstrap=True)\n",
    "       model.fit(X_train, y_train)\n",
    "       pred_bootstrap  = model.predict(X_test)\n",
    "\n",
    "       # Performance\n",
    "       rmse_bootstrap = np.sqrt(mean_squared_error(y_test, pred_bootstrap))\n",
    "\n",
    "       test_dataset['Value_predicted'] = pred_bootstrap\n",
    "       test_dataset['actual_value']    = y_test\n",
    "       test_dataset.sort_values(by=[\"adm1_name\", \"adm2_name\",\"year\"],inplace=True)\n",
    "\n",
    "       test_dataset = restore_trend(test_dataset,model_trend_dict,log_detrend=log_transf, trend_detrend=linear_detrand, normalize_data_mean_std=mean_std_norm)\n",
    "\n",
    "       if plot_y_test:\n",
    "              fig, ax = plt.subplots(ncols=2,nrows=1,figsize=(6,2))\n",
    "              loc = random_plot(test_dataset   , y_axis=\"Value_predicted\",marker='*',ax=ax[0])\n",
    "              _   = random_plot(df_training , y_axis=\"Value-detrand\", marker='*',location=loc,ax=ax[0])\n",
    "              random_plot(test_dataset, y_axis=\"Value_restored\", marker='*',location=loc,ax=ax[1])\n",
    "              random_plot(df_training , y_axis=\"Value\",          marker='*',location=loc,ax=ax[1])\n",
    "\n",
    "       area_loc  = test_dataset[\"Area_prediction\"].to_numpy()\n",
    "       error_loc = test_dataset[\"Value_restored\"].to_numpy() - test_dataset[\"Value\"].to_numpy()\n",
    "\n",
    "       # std_RF    = residuals.std(ddof=1)       \n",
    "\n",
    "       return rmse_bootstrap, len(y_test), model, error_loc, area_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how the model perform we make a plot for a random county (to be inserted into the call to the function **plot_y_test=True**). We are still working with corn yield without trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute National yield we need to weight the yield on every county. Indeed if the yield are comparable but the area are very different the impact on national level will bu much higher for the county with big area. \n",
    "\n",
    "**ASSUMPION** : We assume no particular modeling at this stage for the Area for 2024. We assume it to be known. In a future stage of the modelling, the same techniques applied for corn yield could be applied to the harvested area. It might be the case that harvested area does correspond to planting area and if this is the case then we already have the data since by April (end of our weather database) the planting has already been done. To be sure we should check if there are differences between harvested and planted area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we define the function for the RF training and for the RF prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_training(train_yield_feat_year, features,National=False,plot_y_test=False):\n",
    "    mask_time     = train_yield_feat_year[\"year\"]<YEAR_PREDICTION\n",
    "    df_training   = train_yield_feat_year[mask_time]\n",
    "    model_dict = {}\n",
    "    std_RF = 0\n",
    "    if National:\n",
    "        rmse_total, N_test, model, error_tot, area_tot = random_forest_model(df_training,features,plot_y_test=plot_y_test)\n",
    "        model_dict[\"national\"] = model\n",
    "    else:\n",
    "        mse_list       = []\n",
    "        N_test_total   = 0\n",
    "        area_list      = []\n",
    "        error_list = []\n",
    "\n",
    "        for state_loc in df_training[\"adm1_name\"].unique():\n",
    "            # train the model\n",
    "            rmse, N_test, model, error_loc, area_loc = random_forest_model(df_training[df_training[\"adm1_name\"]==state_loc], features,plot_y_test=plot_y_test)\n",
    "\n",
    "\n",
    "            if error_loc.shape[0]>0:\n",
    "                error_list.append(error_loc)\n",
    "                area_list.append(area_loc)\n",
    "\n",
    "            model_dict[state_loc] = model\n",
    "\n",
    "            # evaluate the global performance for USA\n",
    "            mse_list.append( N_test * rmse**2)\n",
    "            N_test_total = N_test + N_test_total\n",
    "\n",
    "        error_conc    = np.concatenate(error_list)\n",
    "        area_conc     = np.concatenate(area_list)\n",
    "\n",
    "        std_RF        = np.sqrt(np.sum((error_conc*area_conc)**2)/np.sum( area_conc**2 )) # first definition\n",
    "\n",
    "        rmse_total = np.sqrt(1/N_test_total * np.sum(np.array(mse_list)))\n",
    "\n",
    "    return rmse_total, model_dict, std_RF\n",
    "\n",
    "\n",
    "def RF_prediction(model_dict,df_prediction,features):\n",
    "\n",
    "    total_production = 0\n",
    "    total_area       = 0\n",
    "    \n",
    "    for state_loc in df_prediction[\"adm1_name\"].unique():\n",
    "        X_data     = df_prediction[df_prediction[\"adm1_name\"]==state_loc][features]\n",
    "        if X_data.shape[0] > 0:\n",
    "\n",
    "            model = model_dict[state_loc]\n",
    "\n",
    "            prediction     = model.predict(X_data)\n",
    "\n",
    "            # rebuild the dataframe for re-tranding \n",
    "            pred_df  = df_prediction.loc[X_data.index,  [\"year\", \"adm2_name\", \"adm1_name\",\"std\",\"mean\",\"Area_prediction\"]]\n",
    "            pred_df['Value_predicted'] = prediction\n",
    "\n",
    "            # restore the trend of the data\n",
    "            pred_df = restore_trend(pred_df,model_trend_dict,log_detrend=log_transf, trend_detrend=linear_detrand, normalize_data_mean_std=mean_std_norm)\n",
    "\n",
    "            df_prediction.loc[X_data.index, \"predicted_value\"] = pred_df[\"Value_restored\"]\n",
    "\n",
    "            total_production = total_production + np.sum(df_prediction.loc[X_data.index, \"predicted_value\"].to_numpy() * df_prediction.loc[X_data.index, \"Area_prediction\"].to_numpy())\n",
    "            total_area       = total_area + np.sum(df_prediction.loc[X_data.index, \"Area_prediction\"].to_numpy())\n",
    "\n",
    "        else:\n",
    "            print(\"No values in input\")\n",
    "            \n",
    "    corn_yield_tot = total_production/total_area\n",
    "\n",
    "    return df_prediction, corn_yield_tot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we train the RF for the total dataset (without splitting between states) and evaluate the RMSE of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RANDOM_FOREST:\n",
    "    # training bloc\n",
    "    features = ['tmin_min','precip_mean','hot_day', 'swvl2_mean','cold_day', 'max_cons_dry_days','swvl2_std','tmax_mean', 'tmax_max','tmin_mean']\n",
    "\n",
    "    feat_year, train_yield_feat_year  = featuring_training_dataset(months_range=MONTHS_RANGE)\n",
    "    rmse_total, model_dict, _    = random_forest_training(train_yield_feat_year, features, National=True, plot_y_test=False)\n",
    "\n",
    "    print(rmse_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do the same by the training happen state-by-state and it produces a dictionary with the several models that will be used later for forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RANDOM_FOREST:\n",
    "    # training bloc\n",
    "    features = ['tmin_min','precip_mean','hot_day', 'swvl2_mean','cold_day', 'max_cons_dry_days','swvl2_std','tmax_mean', 'tmax_max','tmin_mean']\n",
    "\n",
    "    feat_year, train_yield_feat_year  = featuring_training_dataset(months_range=MONTHS_RANGE)\n",
    "    rmse_total, model_dict, std_RF    = random_forest_training(train_yield_feat_year, features, plot_y_test=False)\n",
    "    print(rmse_total)\n",
    "    print(\"standard deviation is: \"+str(std_RF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We indeed observe that RMSE for the state-by-state model is better than the model trained on the full dataset. We will use the state-by-state model for forecast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the training is more performant when executed for each state undependently, with root mean square error that is 0.155 when all data trained together and 0.132 when the model is trained state by state. Thus for the prediction we use the state-by-state trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before looking at the prevision for the year 2024, let us see how much is the standard deviation for our model. It is computed assuming that the validation set is rapresentative of what could happen in 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction block\n",
    "def prediction(feat_year, year,features):\n",
    "\n",
    "    df_prediction    = feat_year[feat_year[\"year\"]==year]\n",
    "    corn_stats       = (corn_value_tot.drop_duplicates(subset=[\"adm1_name\", \"adm2_name\"])[[\"adm1_name\", \"adm2_name\", \"mean\", \"std\"]])\n",
    "\n",
    "    df_prediction    = df_prediction.merge(corn_stats, on=['adm2_name', 'adm1_name'], how='inner') # done to ensure only cities who have trained are considered. \n",
    "    df_prediction    = df_prediction.merge(area_df_pred, on=['adm2_name', 'adm1_name'], how='inner') # done to ensure only cities who have trained are considered. \n",
    "\n",
    "    df_prediction[\"predicted_value\"] = np.nan\n",
    "    df_prediction,corn_yield_tot    = RF_prediction(model_dict,df_prediction,features)\n",
    "\n",
    "    print(corn_yield_tot)\n",
    "\n",
    "    return df_prediction, corn_yield_tot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we insert 2024 (or whatever) data on which we want to do the forecast. The output of the YEAR_PREDICTION corn yield forecast is shown in the plot below together with its standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RANDOM_FOREST:\n",
    "    df_prediction, corn_yield_tot = prediction(feat_year,YEAR_PREDICTION,features)\n",
    "    df            = download_corn_data()\n",
    "    fig, ax = plt.subplots(ncols=2,nrows=1,figsize=(8,3))\n",
    "    df.plot(x=\"year\",y=\"Value\",ax=ax[0])\n",
    "    ax[0].errorbar(x=[YEAR_PREDICTION],y=[corn_yield_tot], yerr=[std_RF],  fmt=\"*\",  capsize=5, label=\"uncertanty\" )\n",
    "    ax[0].legend([\"production\", \"prevision and uncertanty\"])\n",
    "\n",
    "    df_prediction_2012, corn_yield_tot_2012 = prediction(feat_year,2012,features)\n",
    "    df.plot(x=\"year\",y=\"Value\",ax=ax[1])\n",
    "    ax[1].errorbar(x=[2012],y=[corn_yield_tot_2012], yerr=[std_RF],  fmt=\"*\",  capsize=5, label=\"uncertanty\" )\n",
    "    ax[1].legend([\"production\", \"prevision and uncertanty\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cities who have not been trained are not predicted since there is no trend available and we have seen that the trend is crucial parameter thus we do not know which would be the mean value but we could only know the fluctuations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we create a fucntion to check where two dataframes have different counties and then we check if all couties whose corn yield was available in 2023 have also been forecast for 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given df1 and df2 return a dataframe with elements \"adm1_name\", \"adm2_name\" that are present in df1 but not in df2\n",
    "def missing_groups(df1, df2):\n",
    "    missing_gr_df = df1.merge(df2[[\"adm1_name\", \"adm2_name\"]], on=[\"adm1_name\", \"adm2_name\"],\n",
    "        how=\"left\",indicator=True).query('_merge == \"left_only\"')[[\"adm1_name\", \"adm2_name\"]]\n",
    "    return missing_gr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell acts on the dataframe: it expand the dataframe so that each raw is an input for training (it includes 4 times more input parameters than before because it includes the data of the available month- January to April)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_month_features_new(df):\n",
    "\n",
    "    if features_NN_training == \"all\":\n",
    "        features_wanted = extract_feature_list(feat_year)\n",
    "    else:\n",
    "        features_wanted = features_NN_training\n",
    "        \n",
    "    if (\"month_N\" in df):\n",
    "        mask = df[\"month_N\"].between(*MONTHS_RANGE)\n",
    "        df = df[mask]\n",
    "            \n",
    "        if \"month_N\" in features_wanted:\n",
    "            features_wanted.remove(\"month_N\")\n",
    "\n",
    "        list_to_keep = [el for el in df.columns if not(el in features_wanted) and not(\"month\" in el) ]\n",
    "\n",
    "        print(list_to_keep)\n",
    "        df_wide = df.pivot(index=list_to_keep, columns=\"month_N\", values=features_wanted )\n",
    "\n",
    "        all_features        = [f\"{var}_m{month}\" for var, month in df_wide.columns]\n",
    "\n",
    "        df_wide.columns = [f\"{var}_m{month}\" for var, month in df_wide.columns]\n",
    "\n",
    "        df_wide = df_wide.reset_index()\n",
    "\n",
    "        drop_list = [feat for feat in all_features if df_wide[feat].std() == 0]\n",
    "        features  = [feat for feat in all_features if not(df_wide[feat].std() == 0)]\n",
    "\n",
    "        df_wide.drop(drop_list, axis=1,inplace=True)\n",
    "        \n",
    "        return df_wide, features\n",
    "    else:\n",
    "        return df, features_wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RANDOM_FOREST:\n",
    "    train_df, features                = expand_month_features_new(train_yield_feat_month)\n",
    "    rmse_total, model_dict, std_RF    = random_forest_training(train_df, features, plot_y_test=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do again the prediction with the random forest with the model trained on the monthly aggregate feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RANDOM_FOREST:\n",
    "    df_pred, features             =  expand_month_features_new(feat_month)\n",
    "    df_prediction, corn_yield_tot = prediction(df_pred,YEAR_PREDICTION,features)\n",
    "\n",
    "    df       = download_corn_data()\n",
    "    fig, ax  = plt.subplots(ncols=2,nrows=1,figsize=(7,3))\n",
    "    df.plot(x=\"year\",y=\"Value\",ax=ax[0])\n",
    "    df.plot(x=\"year\",y=\"Value\",ax=ax[1])\n",
    "\n",
    "    ax[0].errorbar(x=[YEAR_PREDICTION],y=[corn_yield_tot], yerr=[std_RF],  fmt=\"*\",  capsize=5, label=\"uncertanty\" )\n",
    "    ax[0].legend([\"production\", \"prevision and uncertanty\"])\n",
    "    ax[1].errorbar(x=[YEAR_PREDICTION],y=[corn_yield_tot], yerr=[std_RF],  fmt=\"*\",  capsize=5, label=\"uncertanty\" )\n",
    "    ax[1].legend([\"production\", \"prevision and uncertanty\"])\n",
    "    ax[1].set_xlim([2000,2025])\n",
    "    print(\"standard deviation is: \"+str(std_RF))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that when trained on the enlarged dataset (that includes also data in different month and not just the yearly aggregated features) the prediction goes up from 190 to 195."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we look for a more sophisticated model for the production. We are going to use deep-learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "has_mps = torch.backends.mps.is_built()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\" # gpu not tested\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_NN = [\"MLP_model\",\"prob_MLP_model\"]  # approach removed[\"LSTM_model\"]\n",
    "\n",
    "model_NN = model_NN[0]\n",
    "\n",
    "yearly_normalization = True\n",
    "\n",
    "with_state_in_the_model = False\n",
    "with_embedding          = False\n",
    "\n",
    "months_range = list(MONTHS_RANGE) # for compatibility with other functions. It should be cleaned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we prepare the dataset for the training and once again we expand our dataset so that each raw is an input with many features per month (there are 4 month January to April)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if features_NN_training == \"all\":\n",
    "    features_wanted = extract_feature_list(feat_month)\n",
    "else:\n",
    "    features_wanted = features_NN_training\n",
    "\n",
    "\n",
    "mask = (train_yield_feat_month[\"year\"] < YEAR_PREDICTION) & (train_yield_feat_month[\"month_N\"].between(*months_range))\n",
    "\n",
    "df = train_yield_feat_month[mask]\n",
    "if \"month_N\" in features_wanted:\n",
    "    features_wanted.remove(\"month_N\")\n",
    "\n",
    "df_wide = df.pivot(index=[\"adm2_name\", \"adm1_name\", \"year\",'Value','Area_prediction', 'Value-detrand'], columns=\"month_N\", values=features_wanted )\n",
    "\n",
    "features_wanted = [f\"{var}_m{month}\" for var, month in df_wide.columns]\n",
    "df_wide.columns = [f\"{var}_m{month}\" for var, month in df_wide.columns]\n",
    "\n",
    "\n",
    "\n",
    "df_wide = df_wide.reset_index()\n",
    "\n",
    "df_train, df_test = train_test_split(df_wide, test_size=0.2, random_state=0)\n",
    "\n",
    "features = []\n",
    "for ff in features_wanted:\n",
    "    if ff in df_train.columns:\n",
    "        if not(df_train[ff].std() == 0):\n",
    "            features.append(ff)\n",
    "print(\"using the following features: \")\n",
    "print(\", \".join(features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_train['state_idx'] = le.fit_transform(df_train['adm1_name'])  # OK\n",
    "df_test['state_idx'] = le.fit_transform(df_test['adm1_name'])  # OK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the right input format for the Pytorch API (the .to(device) call can be cleaned and removed because I could not test it on gpu so it is forced to run on cpu) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels     = ['Value-detrand']\n",
    "group_cols = ['adm2_name',\"adm1_name\",'year']\n",
    "# Create empty list for sequences and keys\n",
    "X_train = []\n",
    "Y_train = []\n",
    "X_test  = []\n",
    "Y_test  = []\n",
    "keys_train = []\n",
    "keys_test  = []\n",
    "\n",
    "# Calcolo media e std solo su train\n",
    "means = df_train[features].mean().to_numpy()\n",
    "stds  = df_train[features].std().to_numpy()\n",
    "\n",
    "# Normalizzazione\n",
    "X_train = (df_train[features].to_numpy() - means) / stds\n",
    "X_test  = (df_test[features].to_numpy()  - means) / stds\n",
    "\n",
    "Y_train = df_train[labels].to_numpy()\n",
    "Y_test  = df_test[labels].to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "if with_state_in_the_model and not(with_embedding):\n",
    "    # one-hot encode state\n",
    "    state_oh_train = pd.get_dummies(df_train['state_idx']).values.astype('float32')\n",
    "    state_oh_test = pd.get_dummies(df_test['state_idx']).reindex(columns=range(len(states)), fill_value=0).values.astype('float32')\n",
    "\n",
    "    # concat weather + state\n",
    "    X_train = np.concatenate([X_train, state_oh_train], axis=1)\n",
    "    X_test = np.concatenate([X_test, state_oh_test], axis=1)\n",
    "\n",
    "    add_dim = 13\n",
    "else:\n",
    "    add_dim = 0 \n",
    "\n",
    "x_train = torch.tensor(X_train, dtype=torch.float32).view(-1, len(features)+add_dim)\n",
    "y_train = torch.tensor(Y_train, dtype=torch.float32).view(-1, 1)\n",
    "x_test  = torch.tensor(X_test, dtype=torch.float32).view(-1, len(features)+add_dim)\n",
    "y_test  = torch.tensor(Y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "if with_embedding:\n",
    "\n",
    "    state_train = torch.tensor(df_train['state_idx'].to_numpy(), dtype=torch.long)\n",
    "    state_test  = torch.tensor(df_test['state_idx'].to_numpy(), dtype=torch.long)\n",
    "    train_dataset = TensorDataset(x_train, state_train, y_train)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(x_test, state_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "else:\n",
    "\n",
    "    # Setup data loaders for batch\n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "    print(x_train.shape)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    test_dataset = TensorDataset(x_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we build the NN (actually I started implementing a second one but I did not finish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "    \n",
    "class MLPModel_old(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x.squeeze(1)  #\n",
    "    \n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_states=0, emb_dim=0):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim  # salva per uso nel forward\n",
    "\n",
    "        if self.emb_dim > 0:\n",
    "            self.state_emb = nn.Embedding(num_states, emb_dim)\n",
    "            input_dim = input_dim + emb_dim\n",
    "        else:\n",
    "            input_dim = input_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, weather_x, state_idx=None):\n",
    "        if self.emb_dim > 0:\n",
    "            state_embed = self.state_emb(state_idx)\n",
    "            x = torch.cat([weather_x, state_embed], dim=1)\n",
    "        else:\n",
    "            x = weather_x\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = self.fc5(x)\n",
    "        return x.squeeze(1)\n",
    "\n",
    "if with_embedding:\n",
    "    model = MLPModel(input_dim=x_train.shape[1], num_states=len(states), emb_dim=4).to(device)\n",
    "else:\n",
    "    model = MLPModel(input_dim=x_train.shape[1]).to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train the model and evaluate the loss on the validation dataset to check if the network is learning or overfitting.\n",
    "\n",
    "We print also the variance of the test set and the variance of the predicted output to be sure that we are not flattening aroung a single mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=1e-2)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=3)\n",
    "\n",
    "epochs = 30\n",
    "early_stop_count = 0\n",
    "min_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        if with_embedding:\n",
    "            x_batch, state_batch, y_batch = batch\n",
    "            outputs = model(x_batch,state_batch)\n",
    "        else:\n",
    "            x_batch, y_batch = batch\n",
    "            outputs = model(x_batch)\n",
    "\n",
    "        y_batch = y_batch.squeeze(-1)  #  for dimension compatibility\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(test_loader):\n",
    "            if with_embedding:\n",
    "                x_batch, state_batch, y_batch = batch\n",
    "                outputs = model(x_batch,state_batch)\n",
    "            else:\n",
    "                x_batch, y_batch = batch\n",
    "                outputs = model(x_batch)\n",
    "            y_batch = y_batch.squeeze(-1)  # for dimension compatibility\n",
    "            loss = criterion(outputs, y_batch)\n",
    "                \n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    val_loss = np.mean(val_losses)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < min_val_loss:\n",
    "        min_val_loss = val_loss\n",
    "        early_stop_count = 0\n",
    "    else:\n",
    "        early_stop_count += 1\n",
    "\n",
    "    if early_stop_count >= 20:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {val_loss:.8f}\")\n",
    "\n",
    "    print(\"Batch output variance:\", torch.var(outputs))\n",
    "    print(\"Batch labels variance:\", torch.var(y_batch))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot training and test dataset versus the corresponding output generated by the network to actually check if the NN has learned or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    if with_embedding:\n",
    "        y_pred_train = model(x_train.to(device),state_train).cpu().numpy()\n",
    "        y_pred_test  = model(x_test.to(device),state_test).cpu().numpy()\n",
    "    else:\n",
    "        y_pred_train = model(x_train.to(device)).cpu().numpy()\n",
    "        y_pred_test  = model(x_test.to(device)).cpu().numpy()\n",
    "\n",
    "    y_train_np   = y_train.cpu().numpy()\n",
    "    y_test_np    = y_test.cpu().numpy()\n",
    "    \n",
    "fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(10,4))\n",
    "ax[1].scatter(y_train_np, y_pred_train, s=5, alpha=0.5)\n",
    "ax[1].set_xlabel(\"True y (train)\")\n",
    "ax[1].set_ylabel(\"Predicted y\")\n",
    "ax[1].set_title(\"Train fit\")\n",
    "ax[1].plot([y_train_np.min(), y_train_np.max()], [y_train_np.min(), y_train_np.max()], 'r--')\n",
    "ax[1].set_xlim([-2,1])\n",
    "\n",
    "ax[0].scatter(y_test_np, y_pred_test, s=5, alpha=0.5)\n",
    "ax[0].set_xlabel(\"True y (test)\")\n",
    "ax[0].set_ylabel(\"Predicted y\")\n",
    "ax[0].set_title(\"Test fit\")\n",
    "ax[0].plot([y_test_np.min(), y_test_np.max()], [y_test_np.min(), y_test_np.max()], 'r--')\n",
    "\n",
    "ax[0].set_xlim([-2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncertanty quantification:\n",
    "Now we estimate the prediction uncertainty using conformal prediction, we compute the absolute errors on the test set and finds the quantile such that only an \"alpha\" fraction exceed it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.05  # 95% coverage\n",
    "model.eval()\n",
    "err_list = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        if with_embedding:        \n",
    "            x_test_batch, state_test_batch, y_test_batch = batch\n",
    "            x_test_batch = x_test_batch.to(device)\n",
    "            state_test_batch = state_test_batch.to(device)\n",
    "            y_test_batch = y_test_batch.to(device).squeeze(-1)\n",
    "            y_pred_test = model(x_test_batch, state_test_batch)\n",
    "        else:\n",
    "            x_test_batch, y_test_batch = batch\n",
    "            x_test_batch = x_test_batch.to(device)\n",
    "            y_test_batch = y_test_batch.to(device).squeeze(-1)\n",
    "            y_pred_test = model(x_test_batch)\n",
    "        err         = torch.abs(y_test_batch - y_pred_test)\n",
    "        err_list.append(err.cpu())\n",
    "\n",
    "err_list = torch.cat(err_list, dim=0).numpy()\n",
    "N_test   = err_list.shape[0]\n",
    "\n",
    "q_level = np.ceil((N_test + 1) * (1 - alpha)) / N_test # useless correction Ntest (used for accounting finite samples)\n",
    "q_cal   = np.quantile(err_list, q_level, method=\"higher\")\n",
    "\n",
    "print(\"Estimate for uncertenty: \"+str((1-alpha)*100)+\"% of the predictions fall within \"+str(q_cal)+ \" of the true values -for NN y-true values dataset-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we apply the NN to the weather dataset of 2024 (or of YEAR_PREDICTION) to get the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    df_prediction_NN    = feat_month[feat_month[\"year\"]==YEAR_PREDICTION]\n",
    "    corn_stats          = (corn_value_tot.drop_duplicates(subset=[\"adm1_name\", \"adm2_name\"])[[\"adm1_name\", \"adm2_name\", \"mean\", \"std\"]])\n",
    "    df_prediction_NN    = df_prediction_NN.merge(corn_stats, on=['adm2_name', 'adm1_name'], how='inner') # done to ensure only cities who have trained are considered. \n",
    "    df_prediction_NN    = df_prediction_NN.merge(area_df_pred, on=['adm2_name', 'adm1_name'], how='inner') # done to ensure only cities who have trained are considered. \n",
    "\n",
    "    features_total = extract_feature_list(feat_month)\n",
    "\n",
    "    df_wide = df_prediction_NN.pivot(index=[\"adm2_name\", \"adm1_name\", \"year\",\"Area_prediction\",\"std\",\"mean\"], columns=\"month_N\", values=features_total )\n",
    "\n",
    "    df_wide.columns = [f\"{var}_m{month}\" for var, month in df_wide.columns]\n",
    "\n",
    "    df_wide = df_wide.reset_index()\n",
    "\n",
    "    means = df_train[features].mean().to_numpy()\n",
    "    stds  = df_train[features].std().to_numpy()\n",
    "\n",
    "    area_YEAR_PREDICTION   = df_wide[\"Area_prediction\"]\n",
    "    state_input            = df_wide[\"adm1_name\"].to_numpy()\n",
    "\n",
    "    if with_embedding:\n",
    "        le = LabelEncoder()\n",
    "        df_wide['state_idx'] = le.fit_transform(df_wide['adm1_name'])  # OK\n",
    "        state_input_pred     = torch.tensor(df_wide['state_idx'].to_numpy(), dtype=torch.long)\n",
    "\n",
    "    X_YEAR_PREDICTION      = (df_wide[features].to_numpy() - means) / stds\n",
    "    x_YEAR_PREDICTION      = torch.tensor(X_YEAR_PREDICTION, dtype=torch.float32).view(-1, len(features))   \n",
    "\n",
    "    if with_embedding:\n",
    "        y_pred_YEAR_PREDICTION = model(x_YEAR_PREDICTION.to(device), state_input_pred).cpu().numpy()\n",
    "    else:\n",
    "        y_pred_YEAR_PREDICTION = model(x_YEAR_PREDICTION.to(device)).cpu().numpy()\n",
    "\n",
    "    range_err   = torch.as_tensor(q_cal).numpy()\n",
    "\n",
    "    corn_yield_tot = []\n",
    "    # this value must be retrended\n",
    "    for val in [y_pred_YEAR_PREDICTION, y_pred_YEAR_PREDICTION + range_err, y_pred_YEAR_PREDICTION - range_err]:\n",
    "        df_wide[\"Value_predicted\"] = val\n",
    "\n",
    "        pred_df = restore_trend(df_wide,model_trend_dict,log_detrend=log_transf, trend_detrend=linear_detrand, normalize_data_mean_std=mean_std_norm)\n",
    "\n",
    "\n",
    "        total_production = np.sum(pred_df[\"Value_restored\"].to_numpy() * pred_df[\"Area_prediction\"].to_numpy())\n",
    "        total_area       = np.sum(pred_df[\"Area_prediction\"].to_numpy())\n",
    "\n",
    "        print(total_production/total_area)\n",
    "\n",
    "        corn_yield_tot.append(total_production/total_area )\n",
    "\n",
    "    if with_embedding:\n",
    "        pred_w_embedding  = corn_yield_tot\n",
    "    else:\n",
    "        pred_wo_embedding = corn_yield_tot\n",
    "\n",
    "\n",
    "#print(pred_w_embedding, pred_wo_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the predicted value and the interval of uncertanty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df       = download_corn_data()\n",
    "fig, ax  = plt.subplots(ncols=1,nrows=1,figsize=(4,3))\n",
    "df.plot(x=\"year\",y=\"Value\",ax=ax)\n",
    "\n",
    "ax.errorbar(x=[YEAR_PREDICTION],y=[corn_yield_tot[0]], yerr=[corn_yield_tot[1]-corn_yield_tot[0]],  fmt=\"*\",  capsize=5, label=\"uncertanty\" )\n",
    "ax.legend([\"production\", \"prevision and uncertanty\"])\n",
    "\n",
    "print(\"The predicted value is: \"+str(corn_yield_tot[0]))\n",
    "print(\"Uupper and lower boundary: \" +str(corn_yield_tot[1]) +\" and \"+ str(corn_yield_tot[2]))\n",
    "# print(\"Estimate for uncertenty: \"+str((1-alpha)*100)+\"% of the predictions fall within \"+str(total_production[2])+ \" of the true values -for NN y-true values dataset-\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_pythorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
